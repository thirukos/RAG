{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8538719,"sourceType":"datasetVersion","datasetId":5100714}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-28T17:00:15.706669Z","iopub.execute_input":"2024-05-28T17:00:15.707019Z","iopub.status.idle":"2024-05-28T17:00:16.067518Z","shell.execute_reply.started":"2024-05-28T17:00:15.706987Z","shell.execute_reply":"2024-05-28T17:00:16.066435Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/one-hundred-years-of-solitude/One_Hundred_Years_of_Solitude.pdf\n","output_type":"stream"}]},{"cell_type":"code","source":"# import torch\n# gpu_memory_bytes = torch.cuda.get_device_properties(0).total_memory\n# gpu_memory_gb = round(gpu_memory_bytes / (2**30))\n# print(f\"Available GPU memory: {gpu_memory_gb} GB\")\n# !nvidia-smi\n\n# '''\n# Available GPU memory: 15 GB\n# Tue May 28 09:39:56 2024       \n# +---------------------------------------------------------------------------------------+\n# | NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n# |-----------------------------------------+----------------------+----------------------+\n# | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n# | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n# |                                         |                      |               MIG M. |\n# |=========================================+======================+======================|\n# |   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n# | N/A   35C    P8               9W /  70W |      3MiB / 15360MiB |      0%      Default |\n# |                                         |                      |                  N/A |\n# +-----------------------------------------+----------------------+----------------------+\n# |   1  Tesla T4                       Off | 00000000:00:05.0 Off |                    0 |\n# | N/A   48C    P8               9W /  70W |      3MiB / 15360MiB |      0%      Default |\n# |                                         |                      |                  N/A |\n# +-----------------------------------------+----------------------+----------------------+\n                                                                                         \n# +---------------------------------------------------------------------------------------+\n# | Processes:                                                                            |\n# |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n# |        ID   ID                                                             Usage      |\n# |=======================================================================================|\n# |  No running processes found                                                           |\n# +---------------------------------------------------------------------------------------+\n# '''","metadata":{"execution":{"iopub.status.busy":"2024-05-28T17:00:16.069513Z","iopub.execute_input":"2024-05-28T17:00:16.069994Z","iopub.status.idle":"2024-05-28T17:00:16.076389Z","shell.execute_reply.started":"2024-05-28T17:00:16.069959Z","shell.execute_reply":"2024-05-28T17:00:16.075331Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# import torch\n# gpu_memory_bytes = torch.cuda.get_device_properties(0).total_memory\n# gpu_memory_gb = round(gpu_memory_bytes / (2**30))\n# print(f\"Available GPU memory: {gpu_memory_gb} GB\")\n# !nvidia-smi\n\n# '''\n# Available GPU memory: 16 GB\n# Tue May 28 09:38:43 2024       \n# +---------------------------------------------------------------------------------------+\n# | NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n# |-----------------------------------------+----------------------+----------------------+\n# | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n# | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n# |                                         |                      |               MIG M. |\n# |=========================================+======================+======================|\n# |   0  Tesla P100-PCIE-16GB           Off | 00000000:00:04.0 Off |                    0 |\n# | N/A   40C    P0              30W / 250W |      2MiB / 16384MiB |      0%      Default |\n# |                                         |                      |                  N/A |\n# +-----------------------------------------+----------------------+----------------------+\n                                                                                         \n# +---------------------------------------------------------------------------------------+\n# | Processes:                                                                            |\n# |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n# |        ID   ID                                                             Usage      |\n# |=======================================================================================|\n# |  No running processes found                                                           |\n# +---------------------------------------------------------------------------------------+\n# '''","metadata":{"execution":{"iopub.status.busy":"2024-05-28T17:00:16.077691Z","iopub.execute_input":"2024-05-28T17:00:16.077963Z","iopub.status.idle":"2024-05-28T17:00:16.092813Z","shell.execute_reply.started":"2024-05-28T17:00:16.077940Z","shell.execute_reply":"2024-05-28T17:00:16.091922Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# requirements\n!pip install fitz\n!pip install PyMuPDF\n!pip install sentence-transformers\n!pip install faiss-cpu\n!pip install tqdm\n!pip install accelerate\n!pip install bitsandbytes\n!pip install flash-attn --no-build-isolation","metadata":{"execution":{"iopub.status.busy":"2024-05-28T17:00:16.094814Z","iopub.execute_input":"2024-05-28T17:00:16.095128Z","iopub.status.idle":"2024-05-28T17:02:22.280192Z","shell.execute_reply.started":"2024-05-28T17:00:16.095074Z","shell.execute_reply":"2024-05-28T17:02:22.279073Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Collecting fitz\n  Downloading fitz-0.0.1.dev2-py2.py3-none-any.whl.metadata (816 bytes)\nCollecting configobj (from fitz)\n  Downloading configobj-5.0.8-py2.py3-none-any.whl.metadata (3.4 kB)\nCollecting configparser (from fitz)\n  Downloading configparser-7.0.0-py3-none-any.whl.metadata (5.4 kB)\nRequirement already satisfied: httplib2 in /opt/conda/lib/python3.10/site-packages (from fitz) (0.21.0)\nRequirement already satisfied: nibabel in /opt/conda/lib/python3.10/site-packages (from fitz) (5.2.1)\nCollecting nipype (from fitz)\n  Downloading nipype-1.8.6-py3-none-any.whl.metadata (6.6 kB)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from fitz) (1.26.4)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from fitz) (2.1.4)\nCollecting pyxnat (from fitz)\n  Downloading pyxnat-1.6.2-py3-none-any.whl.metadata (5.3 kB)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from fitz) (1.11.4)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from configobj->fitz) (1.16.0)\nRequirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /opt/conda/lib/python3.10/site-packages (from httplib2->fitz) (3.1.1)\nRequirement already satisfied: packaging>=17 in /opt/conda/lib/python3.10/site-packages (from nibabel->fitz) (21.3)\nRequirement already satisfied: click>=6.6.0 in /opt/conda/lib/python3.10/site-packages (from nipype->fitz) (8.1.7)\nRequirement already satisfied: networkx>=2.0 in /opt/conda/lib/python3.10/site-packages (from nipype->fitz) (3.2.1)\nCollecting prov>=1.5.2 (from nipype->fitz)\n  Downloading prov-2.0.0-py3-none-any.whl.metadata (3.7 kB)\nRequirement already satisfied: pydot>=1.2.3 in /opt/conda/lib/python3.10/site-packages (from nipype->fitz) (1.4.2)\nRequirement already satisfied: python-dateutil>=2.2 in /opt/conda/lib/python3.10/site-packages (from nipype->fitz) (2.9.0.post0)\nCollecting rdflib>=5.0.0 (from nipype->fitz)\n  Downloading rdflib-7.0.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: simplejson>=3.8.0 in /opt/conda/lib/python3.10/site-packages (from nipype->fitz) (3.19.2)\nCollecting traits!=5.0,<6.4,>=4.6 (from nipype->fitz)\n  Downloading traits-6.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (4.9 kB)\nRequirement already satisfied: filelock>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from nipype->fitz) (3.13.1)\nCollecting etelemetry>=0.2.0 (from nipype->fitz)\n  Downloading etelemetry-0.3.1-py3-none-any.whl.metadata (3.2 kB)\nCollecting looseversion (from nipype->fitz)\n  Downloading looseversion-1.3.0-py2.py3-none-any.whl.metadata (4.6 kB)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->fitz) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->fitz) (2023.4)\nRequirement already satisfied: lxml>=4.3 in /opt/conda/lib/python3.10/site-packages (from pyxnat->fitz) (5.2.1)\nRequirement already satisfied: requests>=2.20 in /opt/conda/lib/python3.10/site-packages (from pyxnat->fitz) (2.31.0)\nCollecting pathlib>=1.0 (from pyxnat->fitz)\n  Downloading pathlib-1.0.1-py3-none-any.whl.metadata (5.1 kB)\nCollecting ci-info>=0.2 (from etelemetry>=0.2.0->nipype->fitz)\n  Downloading ci_info-0.3.0-py3-none-any.whl.metadata (6.1 kB)\nCollecting isodate<0.7.0,>=0.6.0 (from rdflib>=5.0.0->nipype->fitz)\n  Downloading isodate-0.6.1-py2.py3-none-any.whl.metadata (9.6 kB)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.20->pyxnat->fitz) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.20->pyxnat->fitz) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.20->pyxnat->fitz) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.20->pyxnat->fitz) (2024.2.2)\nDownloading fitz-0.0.1.dev2-py2.py3-none-any.whl (20 kB)\nDownloading configobj-5.0.8-py2.py3-none-any.whl (36 kB)\nDownloading configparser-7.0.0-py3-none-any.whl (16 kB)\nDownloading nipype-1.8.6-py3-none-any.whl (3.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading pyxnat-1.6.2-py3-none-any.whl (95 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.6/95.6 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading etelemetry-0.3.1-py3-none-any.whl (6.4 kB)\nDownloading pathlib-1.0.1-py3-none-any.whl (14 kB)\nDownloading prov-2.0.0-py3-none-any.whl (421 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m421.5/421.5 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading rdflib-7.0.0-py3-none-any.whl (531 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m531.9/531.9 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading traits-6.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (5.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m82.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading looseversion-1.3.0-py2.py3-none-any.whl (8.2 kB)\nDownloading ci_info-0.3.0-py3-none-any.whl (7.8 kB)\nDownloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: pathlib, looseversion, traits, isodate, configparser, configobj, ci-info, rdflib, pyxnat, etelemetry, prov, nipype, fitz\nSuccessfully installed ci-info-0.3.0 configobj-5.0.8 configparser-7.0.0 etelemetry-0.3.1 fitz-0.0.1.dev2 isodate-0.6.1 looseversion-1.3.0 nipype-1.8.6 pathlib-1.0.1 prov-2.0.0 pyxnat-1.6.2 rdflib-7.0.0 traits-6.3.2\nCollecting PyMuPDF\n  Downloading PyMuPDF-1.24.4-cp310-none-manylinux2014_x86_64.whl.metadata (3.4 kB)\nCollecting PyMuPDFb==1.24.3 (from PyMuPDF)\n  Downloading PyMuPDFb-1.24.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.4 kB)\nDownloading PyMuPDF-1.24.4-cp310-none-manylinux2014_x86_64.whl (3.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading PyMuPDFb-1.24.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (15.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.8/15.8 MB\u001b[0m \u001b[31m76.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: PyMuPDFb, PyMuPDF\nSuccessfully installed PyMuPDF-1.24.4 PyMuPDFb-1.24.3\nCollecting sentence-transformers\n  Downloading sentence_transformers-3.0.0-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: transformers<5.0.0,>=4.34.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.39.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.66.1)\nRequirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (2.1.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.26.4)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.11.4)\nRequirement already satisfied: huggingface-hub>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.22.2)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (9.5.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2024.2.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2023.12.25)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.3)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.4.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.2.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.15.1->sentence-transformers) (3.1.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\nDownloading sentence_transformers-3.0.0-py3-none-any.whl (224 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.7/224.7 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: sentence-transformers\nSuccessfully installed sentence-transformers-3.0.0\nCollecting faiss-cpu\n  Downloading faiss_cpu-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from faiss-cpu) (1.26.4)\nDownloading faiss_cpu-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m60.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: faiss-cpu\nSuccessfully installed faiss-cpu-1.8.0\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (4.66.1)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.29.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.22.2)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2024.2.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl.metadata (2.2 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.1.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\nDownloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.43.1\nCollecting flash-attn\n  Downloading flash_attn-2.5.9.post1.tar.gz (2.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from flash-attn) (2.1.2)\nCollecting einops (from flash-attn)\n  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (2024.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->flash-attn) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->flash-attn) (1.3.0)\nDownloading einops-0.8.0-py3-none-any.whl (43 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: flash-attn\n  Building wheel for flash-attn (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for flash-attn: filename=flash_attn-2.5.9.post1-cp310-cp310-linux_x86_64.whl size=120576656 sha256=0f3dddbf9bc350ea6b0306ec5ca5fee71b57fe1f06e6b72672690793d9dad2ce\n  Stored in directory: /root/.cache/pip/wheels/cc/ad/f6/7ccf0238790d6346e9fe622923a76ec218e890d356b9a2754a\nSuccessfully built flash-attn\nInstalling collected packages: einops, flash-attn\nSuccessfully installed einops-0.8.0 flash-attn-2.5.9.post1\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport fitz\nfrom tqdm.auto import tqdm\nimport re\nfrom spacy.lang.en import English\nimport pandas as pd\n\nnlp = English()\nnlp.add_pipe(\"sentencizer\")\n\ndef text_formatter(text: str) -> str:\n    cleaned_page = text.replace(\"\\n\", \" \").strip()\n    cleaned_page = cleaned_page[54:].strip() # Removing header in each page\n    page_number_pattern = r'\\s*\\d+\\s*$'\n    cleaned_page = re.sub(page_number_pattern, '', cleaned_page)\n    return cleaned_page\n\ndef split_by_sentence(text):\n    return list(nlp(text).sents)\n\ndef chunking(text_list, slice_size= 5):\n    n = len(text_list)\n    return [text_list[i: i+slice_size] for i in range(0, n, slice_size)]\n\ndef loading_preprocessing(filePath, chunkSize= 5):\n    book = fitz.open(filePath)\n    book_by_pages = []\n    \n    for page_number, page in tqdm(enumerate(book), total=len(book)):\n        text = page.get_text()\n        text = text_formatter(text)\n        sentences = split_by_sentence(text)\n        sentence_chunks = chunking(text_list=sentences, slice_size=chunkSize)\n        \n        book_by_pages.append({\n            \"page_number\": page_number - 6,  # Adjust as needed\n            \"page_char_count\": len(text),\n            \"page_word_count\": len(text.split(\" \")),\n            \"page_sentence_count_raw\": len(text.split(\". \")),\n            \"page_token_count\": len(text) / 4,\n            \"text\": text,\n            \"sentence_chunk\": sentence_chunks,\n            \"num_chunk_per_page\": len(sentence_chunks)\n        })\n    \n    book_by_pages_df = pd.DataFrame(book_by_pages)\n        \n    return book_by_pages_df\n\ndef chunk_to_page_reference(book_by_pages_df):\n    pages_and_chunks = []\n    \n    for _, item in tqdm(book_by_pages_df.iterrows(), total=book_by_pages_df.shape[0]):\n        for sentence_chunk in item[\"sentence_chunk\"]:\n            chunk_dict = {}\n            chunk_dict[\"page_number\"] = item[\"page_number\"]\n            joined_sentence_chunk = \" \".join([str(sent) for sent in sentence_chunk]).replace(\"  \", \" \").strip()\n            joined_sentence_chunk = re.sub(r'\\.([A-Z])', r'. \\1', joined_sentence_chunk)\n            chunk_dict[\"sentence_chunk\"] = joined_sentence_chunk\n\n            chunk_dict[\"chunk_char_count\"] = len(joined_sentence_chunk)\n            chunk_dict[\"chunk_word_count\"] = len(joined_sentence_chunk.split(\" \"))\n            chunk_dict[\"chunk_token_count\"] = len(joined_sentence_chunk) / 4  # 1 token = ~4 characters\n\n            pages_and_chunks.append(chunk_dict)\n            \n    chunks_df = pd.DataFrame(pages_and_chunks)\n    return chunks_df","metadata":{"execution":{"iopub.status.busy":"2024-05-28T17:02:22.281601Z","iopub.execute_input":"2024-05-28T17:02:22.281903Z","iopub.status.idle":"2024-05-28T17:02:26.565637Z","shell.execute_reply.started":"2024-05-28T17:02:22.281873Z","shell.execute_reply":"2024-05-28T17:02:26.564822Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"filePath = '/kaggle/input/one-hundred-years-of-solitude/One_Hundred_Years_of_Solitude.pdf'\nchunkSize = 9 # experimentally determined to make sure 75th percentile of chunk_token_count less than 384\nbook_by_pages_df = loading_preprocessing(filePath, chunkSize= chunkSize)\nbook_by_chunks_df = chunk_to_page_reference(book_by_pages_df)\nprint(\"The 'sentence-transformers' model 'all-mpnet-base-v2' has an input size of 384 tokens (1 token ~= 4 characters in English)\")\nprint(f\"The average number of tokens in a page is {np.mean(book_by_pages_df['page_token_count']).round(2)}\")\nprint(f\"Dividing the pages into chunks of {chunkSize}\")\nprint(f\"The average number of tokens in a chunk is {np.mean(book_by_chunks_df['chunk_token_count']).round(2)}\")\nprint(\"-------------\")\nbook_by_chunks_df.describe().round(2)","metadata":{"execution":{"iopub.status.busy":"2024-05-28T17:02:26.566967Z","iopub.execute_input":"2024-05-28T17:02:26.567547Z","iopub.status.idle":"2024-05-28T17:02:29.816028Z","shell.execute_reply.started":"2024-05-28T17:02:26.567512Z","shell.execute_reply":"2024-05-28T17:02:29.815187Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/201 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e4d829899ec4c3fbf895908587c5481"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/201 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f92c1729b1444cdbf9cac2a8996b969"}},"metadata":{}},{"name":"stdout","text":"The 'sentence-transformers' model 'all-mpnet-base-v2' has an input size of 384 tokens (1 token ~= 4 characters in English)\nThe average number of tokens in a page is 1020.4\nDividing the pages into chunks of 9\nThe average number of tokens in a chunk is 279.18\n-------------\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"       page_number  chunk_char_count  chunk_word_count  chunk_token_count\ncount       727.00            727.00            727.00             727.00\nmean         90.26           1116.73            200.09             279.18\nstd          54.83            507.61             90.96             126.90\nmin          -6.00              2.00              1.00               0.50\n25%          44.00            800.50            144.00             200.12\n50%          85.00           1090.00            196.00             272.50\n75%         136.00           1423.50            254.50             355.88\nmax         194.00           4457.00            827.00            1114.25","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>page_number</th>\n      <th>chunk_char_count</th>\n      <th>chunk_word_count</th>\n      <th>chunk_token_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>727.00</td>\n      <td>727.00</td>\n      <td>727.00</td>\n      <td>727.00</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>90.26</td>\n      <td>1116.73</td>\n      <td>200.09</td>\n      <td>279.18</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>54.83</td>\n      <td>507.61</td>\n      <td>90.96</td>\n      <td>126.90</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>-6.00</td>\n      <td>2.00</td>\n      <td>1.00</td>\n      <td>0.50</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>44.00</td>\n      <td>800.50</td>\n      <td>144.00</td>\n      <td>200.12</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>85.00</td>\n      <td>1090.00</td>\n      <td>196.00</td>\n      <td>272.50</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>136.00</td>\n      <td>1423.50</td>\n      <td>254.50</td>\n      <td>355.88</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>194.00</td>\n      <td>4457.00</td>\n      <td>827.00</td>\n      <td>1114.25</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"book_by_chunks_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-28T17:02:29.817269Z","iopub.execute_input":"2024-05-28T17:02:29.817613Z","iopub.status.idle":"2024-05-28T17:02:29.827974Z","shell.execute_reply.started":"2024-05-28T17:02:29.817579Z","shell.execute_reply":"2024-05-28T17:02:29.827143Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"   page_number                                     sentence_chunk  \\\n0           -6  GABRIEL GARCIA MARQUEZ was born in Aracataca, ...   \n1           -5  Other Avon Bard Books by Gabriel Garcia Marque...   \n2           -4  ONE HUNDRED YEARS OF SOLITUDE  GABRIEL GARCIA ...   \n3           -3  This book was first published in Argentina in ...   \n4           -2         for jomí garcía ascot and maría luisa elío   \n\n   chunk_char_count  chunk_word_count  chunk_token_count  \n0               501                85             125.25  \n1               466                78             116.50  \n2               125                24              31.25  \n3               971               171             242.75  \n4                42                 8              10.50  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>page_number</th>\n      <th>sentence_chunk</th>\n      <th>chunk_char_count</th>\n      <th>chunk_word_count</th>\n      <th>chunk_token_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-6</td>\n      <td>GABRIEL GARCIA MARQUEZ was born in Aracataca, ...</td>\n      <td>501</td>\n      <td>85</td>\n      <td>125.25</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-5</td>\n      <td>Other Avon Bard Books by Gabriel Garcia Marque...</td>\n      <td>466</td>\n      <td>78</td>\n      <td>116.50</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-4</td>\n      <td>ONE HUNDRED YEARS OF SOLITUDE  GABRIEL GARCIA ...</td>\n      <td>125</td>\n      <td>24</td>\n      <td>31.25</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-3</td>\n      <td>This book was first published in Argentina in ...</td>\n      <td>971</td>\n      <td>171</td>\n      <td>242.75</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-2</td>\n      <td>for jomí garcía ascot and maría luisa elío</td>\n      <td>42</td>\n      <td>8</td>\n      <td>10.50</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\nfrom tqdm.auto import tqdm\n\nembedding_model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\", device=\"cuda\")\n\nbook_by_chunks_df['embedding'] = None\n\nfor idx, item in tqdm(book_by_chunks_df.iterrows(), total=book_by_chunks_df.shape[0]):\n    embedding = embedding_model.encode(item[\"sentence_chunk\"], show_progress_bar=False)\n    book_by_chunks_df.at[idx, \"embedding\"] = embedding.tolist()\n\nbook_by_chunks_df.to_pickle('book_by_chunks.pkl')\n","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-05-28T17:02:29.829169Z","iopub.execute_input":"2024-05-28T17:02:29.829430Z","iopub.status.idle":"2024-05-28T17:03:04.353717Z","shell.execute_reply.started":"2024-05-28T17:02:29.829407Z","shell.execute_reply":"2024-05-28T17:03:04.352873Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"2024-05-28 17:02:35.224176: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-28 17:02:35.224287: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-28 17:02:35.357950: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"494f31e8082a4d10b4a6c40bdc78e9c7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d644b602e6f04d6081287f80a9eee801"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f449b5cf07d54bc39bb5f462a5535f7b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb0ed33ab08543e39190e396a234b4b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11359ce52eee41f484347a5c10188a60"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9aea84a7ef1d4dcf8325280c74de24b7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"886440a9c6324633bca6c281e106c552"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8dc403b0f01c427a88adac9698025f52"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"313ec6e70a5041d0b22afdfc624c9bb2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a1a03ca5cc242008fdc959270896e84"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3983ae83a3634667b7065b73a821a13a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/727 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07d756b209f14f08b8a18bd2400bdf1a"}},"metadata":{}}]},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer, util\nimport faiss\nfrom tqdm.auto import tqdm\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom time import perf_counter as timer\nimport textwrap\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer, BitsAndBytesConfig, AutoModelForCausalLM\nfrom transformers.utils import is_flash_attn_2_available \nfrom huggingface_hub import login\n\nlogin(token='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')\n\nbook_by_chunks_df = pd.read_pickle(\"/kaggle/working/book_by_chunks.pkl\")\n\nembedding_model = SentenceTransformer('all-mpnet-base-v2', device='cuda')\n\ndef print_wrapped(text, wrap_length=80):\n    wrapped_text = textwrap.fill(text, wrap_length)\n    print(wrapped_text)\n\ndef print_top_results(indices, scores, df):\n    for idx, score in zip(indices, scores):\n        page_number = df.at[idx, \"page_number\"]\n        sentence_chunk = df.at[idx, \"sentence_chunk\"]\n        print(f\"\\nScore: {score:.4f}\")\n        print(f\"Page Number: {page_number}\")\n        print_wrapped(f\"Sentence Chunk: {sentence_chunk}\")\n\ndef dot_score(query, book_embedding, topk=5):\n    query_embedding = embedding_model.encode(query, convert_to_tensor=True, show_progress_bar=False)\n    start_time = timer()\n    dot_scores = util.dot_score(a=query_embedding, b=book_embedding)[0]\n    end_time = timer()\n    top_results_dot_product = torch.topk(dot_scores, k=topk)\n    top_indices = top_results_dot_product.indices.cpu().numpy()\n    top_scores = top_results_dot_product.values.cpu().numpy()\n    return top_indices, top_scores, end_time - start_time\n\ndef faiss_score(query, book_embedding, topk=5):\n    embeddings_np = book_embedding.cpu().numpy().astype('float32')\n    index = faiss.IndexFlatL2(embeddings_np.shape[1])\n    index.add(embeddings_np)\n    query_embedding = embedding_model.encode(query, convert_to_tensor=True, show_progress_bar=False).cpu().numpy().astype('float32')\n    start_time = timer()\n    distances, indices = index.search(query_embedding.reshape(1, -1), topk)\n    end_time = timer()\n    return indices[0], distances[0], end_time - start_time\n\ndef define_llm_model(model_id = 'google/flan-t5-large', use_quantization_config=False):\n    quantization_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_compute_dtype=torch.float16,\n        bnb_4bit_use_double_quant=True,\n        bnb_4bit_quant_type=\"nf4\"\n    ) if use_quantization_config else None\n\n    if model_id == 'google/gemma-2b-it':\n        llm_model = AutoModelForCausalLM.from_pretrained(\n            pretrained_model_name_or_path=model_id,\n            torch_dtype=torch.float16,\n            quantization_config=quantization_config if use_quantization_config else None,\n            low_cpu_mem_usage=False)\n    else:\n        if quantization_config:\n            llm_model = AutoModelForSeq2SeqLM.from_pretrained(\n                model_id,\n                quantization_config=quantization_config,\n                device_map=\"auto\"\n            )\n    #         .to('cuda')\n        else:\n            llm_model = AutoModelForSeq2SeqLM.from_pretrained(model_id).to('cuda')\n\n    tokenizer = AutoTokenizer.from_pretrained(model_id)\n\n    if is_flash_attn_2_available():\n        llm_model.gradient_checkpointing_enable()\n        print(\"Flash Attention v2 is available and enabled.\")\n    else:\n        print(\"Flash Attention v2 is not available.\")\n\n    return llm_model, tokenizer\n\ndef retrieve_relevant_resources(query, embeddings, n_resources_to_return=5, method='faiss', print_time=False):\n    if method == 'faiss':\n        indices, scores, time_taken = faiss_score(query, embeddings, topk=n_resources_to_return)\n    else:\n        indices, scores, time_taken = dot_score(query, embeddings, topk=n_resources_to_return)\n    \n    if print_time:\n        print(f\"[INFO] Time taken to get scores on {len(embeddings)} embeddings: {time_taken:.5f} seconds.\")\n    \n    return indices, scores\n\ndef prompt_formatter(query, context_items):\n    context = \"- \" + \"\\n- \".join([item[\"sentence_chunk\"] for item in context_items])\n    base_prompt = \"\"\"Based on the following context items, please answer the query.\n    Give yourself room to think by extracting relevant passages from the context before answering the query.\n    Don't return the thinking, only return the answer.\n    Make sure your answers are as explanatory as possible.\n    Use the following examples as reference for the ideal answer style.\n    \\nExample 1:\n    Query: Who is José Arcadio Buendía?\n    Answer: José Arcadio Buendía is one of the central characters in \"One Hundred Years of Solitude.\" He is the patriarch of the Buendía family and the founder of the town of Macondo. José Arcadio is known for his curiosity and inventiveness, often delving into scientific and alchemical experiments. His character represents the themes of ambition and the quest for knowledge, which are central to the novel's narrative.\n    \\nExample 2:\n    Query: What is the significance of the gypsies in \"One Hundred Years of Solitude\"?\n    Answer: The gypsies in \"One Hundred Years of Solitude\" play a crucial role in introducing the inhabitants of Macondo to new ideas, inventions, and magical elements. Melquíades, a gypsy, becomes a close friend of José Arcadio Buendía and brings knowledge and mystical artifacts to the town. The gypsies symbolize the outside world's influence on Macondo and the blending of reality and fantasy in the novel.\n    \\nNow use the following context items to answer the user query:\n    {context}\n    \\nRelevant passages: <extract relevant passages from the context here>\n    User query: {query}\n    Answer:\"\"\"\n \n    base_prompt = base_prompt.format(context=context, query=query)\n    return base_prompt\n\ndef ask(query, temperature=0.7, max_new_tokens=512, format_answer_text=True, return_answer_only=True):\n    indices, scores = retrieve_relevant_resources(query=query, embeddings=embeddings)\n    context_items = [book_by_chunks_df.iloc[int(i)].to_dict() for i in indices]\n\n    for i, item in enumerate(context_items):\n        item[\"score\"] = scores[i]\n        \n    prompt = prompt_formatter(query=query, context_items=context_items)\n    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n\n    outputs = llm_model.generate(**input_ids, temperature=temperature, do_sample=True, max_new_tokens=max_new_tokens)\n    output_text = tokenizer.decode(outputs[0])\n\n    if format_answer_text:\n        output_text = output_text.replace(prompt, \"\").replace('<bos>', '').replace('<eos>', '')\n#         output_text = output_text.replace(prompt, \"\").replace(\"<pad>\", \"\").replace(\"</s>\", \"\")\n\n    if return_answer_only:\n        return output_text, context_items\n    \n    return output_text, context_items","metadata":{"execution":{"iopub.status.busy":"2024-05-28T18:35:45.496065Z","iopub.execute_input":"2024-05-28T18:35:45.496512Z","iopub.status.idle":"2024-05-28T18:35:46.845868Z","shell.execute_reply.started":"2024-05-28T18:35:45.496479Z","shell.execute_reply":"2024-05-28T18:35:46.844866Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\nToken is valid (permission: read).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"embeddings = torch.tensor(np.array(book_by_chunks_df[\"embedding\"].to_list()), dtype=torch.float32).to('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Define and load the LLM model with quantization\n# Choose either 'google/gemma-2b-it' or 'google/flan-t5-large'\nmodel_id = 'google/gemma-2b-it'\nuse_quantization = True\nllm_model, tokenizer = define_llm_model(model_id, use_quantization)\n\nquery = \"Author's name?\"\nprint(f\"Query: '{query}'\\n\")\n\n# lower temperature = more deterministic outputs, higher temperature = more creative outputs\nanswer, context_items = ask(query=query, temperature=0.7, max_new_tokens=512, return_answer_only=True)\nprint(\"\\nAnswer:\\n\")\nprint_wrapped(answer)\nprint(\"\\nContext items:\")\nfor item in context_items:\n    print_wrapped(f\"Page {item['page_number']}: {item['sentence_chunk']}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-28T18:35:48.902578Z","iopub.execute_input":"2024-05-28T18:35:48.902945Z","iopub.status.idle":"2024-05-28T18:35:59.327648Z","shell.execute_reply.started":"2024-05-28T18:35:48.902912Z","shell.execute_reply":"2024-05-28T18:35:59.326673Z"},"trusted":true},"execution_count":33,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b60bd8885b4b47729a589a73e3f7ae27"}},"metadata":{}},{"name":"stdout","text":"Flash Attention v2 is available and enabled.\nQuery: 'Author's name?'\n\n\nAnswer:\n\n The author's name is Gabriel Garcia Marquez.\n\nContext items:\nPage -4: ONE HUNDRED YEARS OF SOLITUDE  GABRIEL GARCIA MARQUEZ  TRANSLATED FROM\nTHE SPANISH BY GREGORY RABASSA   AVON BOOKS – NEW YORK\nPage 190: Seeing him lost in the labyrinths of kinship, trembling with\nuncertainty, the arth- ritic priest, who was watching him from his hammock,\nasked him compassionately what his name was. “Aureliano Buendía,” he said. “Then\ndon’t wear yourself out searching,” the priest exclaimed with final conviction.\n“ Many years ago there used to be a street here with that name and in those days\npeople had the custom of naming their children after streets.” Aureliano\ntrembled with rage. “So!” he said. “ You don’t believe it either.” “Believe\nwhat?”\nPage 152: He had the patience to listen to her for a whole day until he caught\nher in a slip. Fernanda did not pay him any mind, but she lowered her voice.\nThat night at dinner the exasperating buzzing of the singsong had conquered the\nsound of the rain. Aureliano, Segundo ate very little, with his head down, and\nhe went to his room early. At breakfast on the following day Fernanda was\ntrembling, with a look of not having slept well, and she seemed completely\nexhausted by her rancor. Never- theless, when her husband asked if it was not\npossible to have a soft-boiled egg, she did not answer simply that they had run\nout of eggs the week before, but she worked up a violent diatribe against men\nwho spent their time contemplating their navels and then had the gall to ask for\nlarks’ livers at the table. Aureliano Segundo took the children to look at the\nencyclopedia, as always, and Fernanda pretended to straighten out Meme’s room\njust so that he could listen to her muttering, of course, that it certainly took\ncheek for him to tell the poor innocents that there was a picture of Colonel\nAureliano Buendía in the encyclopedia. During the afternoon, while the children\nwere having their nap, Aureliano Segundo sat on the porch and Fernanda pursued\nhim even there, provoking him, tormenting him, hovering about him with her\nimplacable horsefly buzzing, saying that, of course, while there was nothing to\neat except stones, her husband was sitting there like a sultan of Persia,\nwatching it rain, because that was all he was, a slob, a sponge, a good-for-\nnothing, softer than cotton batting, used to living off women and convinced that\nhe had married Jonah’s wife, who was so content with the story of the whale.\nAureliano Segundo listened to her for more than two hours, impassive, as if he\nwere deaf.\nPage -5: Other Avon Bard Books by Gabriel Garcia Marquez  THE AUTUMN OF THE\nPATRIARCH IN EVIL HOUR  Avon Books are available at special quantity discounts\nfor bulk purchases for sales promotions, premiums, fund raising or educational\nuse. Special books, or book excerpts, can also be created to fit specific needs.\nFor details write or telephone the office of the Director of Special Markets,\nAvon Books, Dept. FP, 105 Madison Avenue, New York, New York 10016,\n212-481-5653.\nPage 12: She calmly placed her cards on an old carpenter’s bench. saying\nanything that came into her head, while the boy waited beside her, more bored\nthan intrigued. Suddenly she reached out her hand and touched him. “ Lordy!” she\nsaid, sincerely startled, and that was all she could say. José Arcadio felt his\nbones filling up with foam, a languid fear, and a terrible desire to weep. The\nwoman made no insinuations. But José Arcadio kept looking for her all night\nlong, for the smell of smoke that she had under her armpits and that had got\ncaught under his skin. He wanted to be with her all the time, he wanted her to\nbe his mother, for them never to leave the granary, and for her to say “Lordy!”\n","output_type":"stream"}]},{"cell_type":"code","source":"# query = \"Who is the gypsy traveller?\"\n# print(f\"Query: '{query}'\\n\")\n# embeddings = torch.tensor(np.array(book_by_chunks_df[\"embedding\"].to_list()), dtype=torch.float32).to('cuda' if torch.cuda.is_available() else 'cpu')\n\n# dot_top_indices, dot_top_scores, dot_time_taken = dot_score(query=query, book_embedding=embeddings)\n# print(f\"Time taken to get scores on {len(embeddings)} embeddings: {dot_time_taken:.5f} seconds.\")\n\n# faiss_top_indices, faiss_top_scores, faiss_time_taken = faiss_score(query, embeddings)\n# print(f\"Time taken to get scores on {len(embeddings)} embeddings: {faiss_time_taken:.5f} seconds.\")\n# ############\n# model_id = 'google/flan-t5-large'\n# use_quantization = True\n# llm_model, tokenizer = define_llm_model(model_id, use_quantization)\n# ############\n# query = \"Who is the author?\"\n# print(f\"Query: {query}\")\n\n# # Answer query with context and return context \n# answer, context_items = ask(query=query, temperature=0.7, max_new_tokens=512, return_answer_only=True)\n\n# print(f\"Answer:\\n\")\n# print_wrapped(answer)","metadata":{"execution":{"iopub.status.busy":"2024-05-28T17:03:23.860282Z","iopub.status.idle":"2024-05-28T17:03:23.860661Z","shell.execute_reply.started":"2024-05-28T17:03:23.860489Z","shell.execute_reply":"2024-05-28T17:03:23.860505Z"},"trusted":true},"execution_count":null,"outputs":[]}]}